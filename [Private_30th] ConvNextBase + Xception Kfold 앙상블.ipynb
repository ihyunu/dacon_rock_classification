{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4ezaibD-QM2",
        "outputId": "d5c28359-3ad1-47d1-9e03-4acf44ca518f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/MyDrive/open.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "데이터 로드 및 데이터프레임 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from tensorflow.keras.applications import Xception, ConvNeXtBase\n",
        "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import Sequence\n",
        "import albumentations as A\n",
        "\n",
        "def Create_DataFrame(train_dir, test_dir, test_csv):\n",
        "    train_paths, train_labels = [], []\n",
        "    for root, _, files in os.walk(train_dir):\n",
        "        for f in files:\n",
        "            train_paths.append(os.path.join(root,f))\n",
        "            train_labels.append(os.path.basename(root))\n",
        "    train_df = pd.DataFrame({'img_path':train_paths, 'label':train_labels})\n",
        "\n",
        "    test_paths = []\n",
        "    for root, _, files in os.walk(test_dir):\n",
        "        for f in sorted(files):\n",
        "            test_paths.append(os.path.join(root,f))\n",
        "    test_df = pd.read_csv(test_csv)\n",
        "    test_df['img_path'] = test_paths\n",
        "    return train_df, test_df\n",
        "\n",
        "train_df, test_df = Create_DataFrame(\n",
        "    '/content/open/train',\n",
        "    '/content/open/test',\n",
        "    '/content/open/test.csv'\n",
        ")\n",
        "class_names = sorted(train_df['label'].unique())\n",
        "n_classes = len(class_names)\n",
        "label2idx = {c:i for i,c in enumerate(class_names)}\n",
        "y = train_df['label'].map(label2idx).values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "시퀀스 데이터셋"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class Stone_Dataset(Sequence):\n",
        "    def __init__(self, paths, labels, batch_size=64, augmentor=None):\n",
        "        self.paths, self.labels = paths, labels\n",
        "        self.bs = batch_size\n",
        "        self.aug = augmentor\n",
        "    def __len__(self): return int(np.ceil(len(self.paths)/self.bs))\n",
        "    def __getitem__(self, idx):\n",
        "        batch = self.paths[idx*self.bs:(idx+1)*self.bs]\n",
        "        X = np.zeros((len(batch),224,224,3),dtype=np.float32)\n",
        "        for i,p in enumerate(batch):\n",
        "            img = cv2.cvtColor(cv2.imread(p),cv2.COLOR_BGR2RGB)\n",
        "            if self.aug: img = self.aug(image=img)['image']\n",
        "            img = cv2.resize(img,(224,224)) / 255.\n",
        "            X[i]=img\n",
        "        if self.labels is None: return X\n",
        "        lbl = self.labels[idx*self.bs:(idx+1)*self.bs]\n",
        "        y_onehot = np.eye(n_classes)[lbl]\n",
        "        return X, y_onehot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "모델 빌더"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def build(backbone):\n",
        "    inp = Input((224,224,3))\n",
        "    base = backbone(include_top=False, weights='imagenet', input_tensor=inp)\n",
        "    x = GlobalAveragePooling2D()(base.output)\n",
        "    out = Dense(n_classes, activation='softmax')(x)\n",
        "    return Model(inp,out)\n",
        "\n",
        "# 미리 학습된 가중치 경로\n",
        "xp_w = '/content/gdrive/MyDrive/Xception10-0.31.weights.h5'\n",
        "cn_w = '/content/gdrive/MyDrive/ConvNeXtBase06-0.27.weights.h5'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "미리 학습된 모델가중치의 Kfold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "NF = 5\n",
        "skf = StratifiedKFold(n_splits=NF, shuffle=True, random_state=42)\n",
        "\n",
        "oof_x = np.zeros((len(train_df), n_classes), dtype=np.float32)\n",
        "oof_c = np.zeros_like(oof_x)\n",
        "\n",
        "for fold,(tr_idx,va_idx) in enumerate(skf.split(train_df, y),1):\n",
        "    print(f\"\\n-- Fold {fold}\")\n",
        "    # 데이터\n",
        "    tr_paths = train_df['img_path'].iloc[tr_idx].values\n",
        "    va_paths = train_df['img_path'].iloc[va_idx].values\n",
        "    tr_lbl   = y[tr_idx]\n",
        "    va_lbl   = y[va_idx]\n",
        "\n",
        "    tr_ds = Stone_Dataset(tr_paths, tr_lbl, batch_size=64, augmentor=A.HorizontalFlip(p=0.5))\n",
        "    va_ds = Stone_Dataset(va_paths, va_lbl, batch_size=64, augmentor=None)\n",
        "\n",
        "    # Xception\n",
        "    m_x = build(Xception);  m_x.load_weights(xp_w)\n",
        "    preds_va_x = m_x.predict(va_ds, verbose=0)\n",
        "    # ConvNeXtBase\n",
        "    m_c = build(ConvNeXtBase); m_c.load_weights(cn_w)\n",
        "    preds_va_c = m_c.predict(va_ds, verbose=0)\n",
        "\n",
        "    oof_x[va_idx] = preds_va_x\n",
        "    oof_c[va_idx] = preds_va_c\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "앙상블 하기위해 최적의 가중치 검색"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_w, best_f1 = 0, 0\n",
        "for w in np.linspace(0,1,101):\n",
        "    ens = w*oof_x + (1-w)*oof_c\n",
        "    pred = ens.argmax(axis=1)\n",
        "    f1 = f1_score(y, pred, average='macro')\n",
        "    if f1>best_f1:\n",
        "        best_f1, best_w = f1, w\n",
        "\n",
        "print(f\"\\n▶ Best macro-F1 on OOF: {best_f1:.4f} (w_x={best_w:.2f}, w_c={1-best_w:.2f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OOF 앙상블 확률에 대해 클래스별 Isotonic 캘리브레이션\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "oof_ens = best_w*oof_x + (1-best_w)*oof_c\n",
        "iso_models = []\n",
        "# for each class j, fit iso on [p_j] vs [y==j]\n",
        "for j in range(n_classes):\n",
        "    ir = IsotonicRegression(out_of_bounds='clip')\n",
        "    ir.fit(oof_ens[:,j], (y==j).astype(int))\n",
        "    iso_models.append(ir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "탐색한 가중치 적용하여 앙상블"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_ds = Stone_Dataset(test_df['img_path'].values, None, batch_size=64, augmentor=None)\n",
        "\n",
        "# fold-averaged test preds\n",
        "preds_x = np.zeros((len(test_df), n_classes),dtype=np.float32)\n",
        "preds_c = np.zeros_like(preds_x)\n",
        "for fold in range(NF):\n",
        "    # 그냥 reuse same loaded models (weights 고정)\n",
        "    pass\n",
        "# (사실 모델마다 한 번만 predict 해도 되므로:)\n",
        "preds_x = build(Xception).load_weights(xp_w) or build(Xception).predict(test_ds,verbose=0)\n",
        "# 위 한 줄 대신:\n",
        "m_x = build(Xception); m_x.load_weights(xp_w)\n",
        "m_c = build(ConvNeXtBase); m_c.load_weights(cn_w)\n",
        "preds_x = m_x.predict(test_ds, verbose=0)\n",
        "preds_c = m_c.predict(test_ds, verbose=0)\n",
        "\n",
        "ens_test = best_w*preds_x + (1-best_w)*preds_c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "보정적용(근데 보정 안해도 될거같음..)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZg1e-uHXMZJ",
        "outputId": "e7dd7cc9-1f79-459b-8ad4-1a208d2c1d20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "-- Fold 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "-- Fold 2\n",
            "\n",
            "-- Fold 3\n",
            "\n",
            "-- Fold 4\n",
            "\n",
            "-- Fold 5\n",
            "\n",
            "▶ Best macro-F1 on OOF: 0.9768 (w_x=0.48, w_c=0.52)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Done: submit_cv_blend_calibrated.csv\n"
          ]
        }
      ],
      "source": [
        "# 캘리브레이션 적용\n",
        "calib = np.zeros_like(ens_test)\n",
        "for j,ir in enumerate(iso_models):\n",
        "    calib[:,j] = ir.predict(ens_test[:,j])\n",
        "# row-wise 정규화\n",
        "calib = calib / calib.sum(axis=1, keepdims=True)\n",
        "\n",
        "# 최종 레이블\n",
        "final_idx = calib.argmax(axis=1)\n",
        "final_labels = [class_names[i] for i in final_idx]\n",
        "\n",
        "sub = pd.read_csv('/content/open/sample_submission.csv')\n",
        "sub['rock_type'] = final_labels\n",
        "sub.to_csv('submit_cv_blend_calibrated.csv', index=False)\n",
        "\n",
        "print(\"✅ Done: submit_cv_blend_calibrated.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
